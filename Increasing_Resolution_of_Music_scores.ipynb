{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "'''\n",
    "This talk was from Sam tensorfow talk. \n",
    "Code heavily adapted from https://github.com/samwit/TensorFlowTalks/blob/master/talk3/Super%20Resolution%20for%20TF%20Talk-GPU-2k-8x-Presentation.ipynb\n",
    "\n",
    "Super Resolution using Perceptual Loss\n",
    "Based on \"Perceptual Losses for Real-Time Style Transfer and Super-Resolution\" by Johnson et.al http://arxiv.org/abs/1603.08155\n",
    "\n",
    "Rationale : Improving image resolution with apparant 'pixelation' in a quick and fast manner. \n",
    "Time taken : 1hr to train and source images. \n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import warnings\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.layers import Flatten, Reshape, Dropout, merge, Lambda\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv2D,Conv2DTranspose,UpSampling2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K\n",
    "#from imagenet_utils import decode_predictions\n",
    "#from imagenet_utils import _obtain_input_shape\n",
    "import tensorflow as tf\n",
    "#import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras import optimizers\n",
    "import keras\n",
    "\n",
    "print('TensorFlow:',tf.__version__)\n",
    "print('Keras:',keras.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_hr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "''' \n",
    "defining  the upsampling blocks\n",
    "'''\n",
    "def conv_block(x, num_filters, filter_size, stride=(2,2), mode='same', act=True):\n",
    "    x = Conv2D(num_filters, (filter_size, filter_size), strides=stride, padding=mode)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    return Activation('relu')(x) if act else x\n",
    "\n",
    "def res_block(initial_input, num_filters=64):\n",
    "    x = conv_block(initial_input, num_filters, 3, (1,1))\n",
    "    x = conv_block(x, num_filters, 3, (1,1), act=False)\n",
    "    return merge([x, initial_input], mode='sum')\n",
    "\n",
    "# Up Sampling block aka deconvolution\n",
    "def up_block(x, num_filters, size):\n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2D(num_filters, (size, size), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    return Activation('relu')(x)\n",
    "\n",
    "# Load data\n",
    "arr_hr = np.load('High_res.npy')\n",
    "arr_lr = np.load('Low_res.npy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "Music scores , Original (Left), Pixelated (Right)\n",
    "The scores were resized using python cv2 command using bilinear interpolation\n",
    "to obtain train and test sets. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(30,20))\n",
    "axs[0].imshow(arr_hr[0]);\n",
    "axs[1].imshow(arr_lr[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(30,20))\n",
    "axs[0].imshow(arr_hr[-100]);\n",
    "axs[1].imshow(arr_lr[-100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This is out upscale model\n",
    "def get_upsampling_model_2x(arr_lr):\n",
    "    inp=Input(arr_lr.shape[1:])\n",
    "    x=conv_block(inp, 64, 9, (1,1))\n",
    "    x=res_block(x)\n",
    "    x=res_block(x)\n",
    "    x=res_block(x)\n",
    "    x=res_block(x)\n",
    "    x=up_block(x, 64, 3)\n",
    "    x=Conv2D(3, (9, 9), activation='tanh', padding='same')(x)\n",
    "    outp=Lambda(lambda x: (x+1)*127.5)(x) # this restores to a normal image\n",
    "    return inp,outp\n",
    "'''\n",
    "Notes\n",
    "\n",
    "1x1 convs with large 9 lets us have a larger receptive field\n",
    "\n",
    "The Tanh gets us between -1,1\n",
    "+1 * 127.5 gets the 0-255 back\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# this gets the output from the model\n",
    "upsampled_inp,upsampled_output = get_upsampling_model_2x(arr_lr) \n",
    "\n",
    "# we define the tensor sizes by giving it the example tensor\n",
    "up_model2 = Model(upsampled_inp,upsampled_output)\n",
    "up_model2.summary() #upscales from 180*120*3 to 360*240*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We will now use the VGG pre_trained model (image classfication model) to get the loss, \n",
    "and will elaborate in the next section\n",
    "'''\n",
    "#vgg input \n",
    "vgg_inp=Input(arr_hr.shape[1:])\n",
    "\n",
    "#vgg network\n",
    "vgg= keras.applications.VGG16(include_top=False, input_tensor=vgg_inp)\n",
    "for l in vgg.layers: l.trainable=False  # can't have loss function be trainable\n",
    "\n",
    "# Lambda makes a layer of a function/ this makes the preprocessing a layer\n",
    "# This preprocesses our normal images to make them ready for VGG \n",
    "#preproc_layer = Lambda(preproc)\n",
    "\n",
    "\n",
    "# get the vgg output \n",
    "vgg_out_layer = vgg.get_layer('block2_conv2').output\n",
    "\n",
    "# making model Model(inputs, outputs)\n",
    "vgg_content = Model(vgg_inp, vgg_out_layer)\n",
    "\n",
    "vgg_content.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We will feed the predicted images and original from block2_conv2 (Conv2D) ,\n",
    "and minimize the mse of outputs between the high resolution and predicted image  \n",
    "'''\n",
    "\n",
    "# this is the VGG model with the HR input\n",
    "vgg_hr_image = vgg_content(vgg_inp)\n",
    "\n",
    "# this is the upsampled network\n",
    "vgg_upsampled_output = vgg_content(upsampled_output)\n",
    "\n",
    "\n",
    "\n",
    "# Mean Sum of Squared Errors on the outputs of the 2 VGG nets\n",
    "loss = Lambda(lambda x: K.sqrt(K.mean((x[0]-x[1])**2, (1,2))))\\\n",
    "       ([vgg_hr_image, vgg_upsampled_output])\n",
    "\n",
    "super_res_model = Model([upsampled_inp, vgg_inp], loss)\n",
    "super_res_model.compile('adam', 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# we need a target for the network to train to\n",
    "targ = np.zeros((arr_hr.shape[0], 128))\n",
    "# define batch_generator since images are too large to fit into memory\n",
    "def batch_generator(X, y, batch_size,shuffle=True):\n",
    "    np.random.seed(2)\n",
    "    number_of_batches = y.shape[0]/batch_size\n",
    "    counter=0\n",
    "    shuffle_index = np.arange(y.shape[0])\n",
    "    if shuffle is True:\n",
    "        np.random.shuffle(shuffle_index,)\n",
    "    X[0] =  X[0][shuffle_index, :,:,:]\n",
    "    X[1] =  X[1][shuffle_index, :,:,:]\n",
    "    y =  y[shuffle_index,:]\n",
    "    while 1:\n",
    "        index_batch = shuffle_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch0 = X[0][index_batch,:,:,:]\n",
    "        X_batch1 = X[1][index_batch,:,:,:]\n",
    "        y_batch = y[index_batch,:]\n",
    "        counter += 1\n",
    "        if counter >= 1*len(y)//batch_size-1:\n",
    "            counter = 0\n",
    "            np.random.shuffle(shuffle_index,)\n",
    "        yield([np.array(X_batch0),np.array(X_batch1)],np.array(y_batch))\n",
    "# split 50 train 50 test\n",
    "tr_generator = batch_generator([arr_lr[0::2], arr_hr[0::2]],targ[0::2],1) #yes i know its only 1\n",
    "val_generator = batch_generator([arr_lr[1::2], arr_hr[1::2]],targ[1::2],2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model_path = 'keras_model.h5'\n",
    "callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3, # was 10\n",
    "            verbose=0),\n",
    "        \n",
    "        ModelCheckpoint(\n",
    "            model_path, \n",
    "            monitor='val_loss',\n",
    "            save_best_only=True, \n",
    "            verbose=0)\n",
    "    ]\n",
    "\n",
    "super_res_model.fit_generator(tr_generator,callbacks=callbacks,\n",
    "                                  steps_per_epoch=len(arr_lr[0::2])//2,\n",
    "                                  epochs=20,\n",
    "                                  verbose=1,\n",
    "                                  validation_data=val_generator,\n",
    "                                  validation_steps=len(arr_lr[1::2])//2)\n",
    "'''\n",
    "I paused early in notebook, in actuality i ran on 1200 pictures dataset once. \n",
    "Val loss was about 70K. Cross validation was done with my EYES since i wanted a quick solution\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# saving up_model2 weights, not saving the VGG ones\n",
    "up_model2.save_weights('./save_weights.h5')\n",
    "up_model2.load_weights('./temp_x2_69k.h5') #Fully trained model outside the notebook \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# this gets the output from the model\n",
    "upsampled_inp,upsampled_output = get_upsampling_model_2x(arr_hr) \n",
    "\n",
    "# we define the tensor sizes by giving it the example tensor\n",
    "up_model3 = Model(upsampled_inp,upsampled_output)\n",
    "up_model3.load_weights('./temp_x2_69k.h5')\n",
    "up_model3.save('model.h5')\n",
    "x = up_model2.predict(arr_lr[-100:-99])\n",
    "y = up_model3.predict(arr_hr[-100:-99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Now we try improving the resolution of the original image to 2x the size\n",
    "'''\n",
    "fig, axs = plt.subplots(2,2,figsize=(50,50))\n",
    "axs[0,0].imshow(arr_lr[-100][85:160,0:75,:]);\n",
    "axs[0,1].imshow(x[0].astype(np.uint8)[170:320,0:150,:]);\n",
    "axs[1,0].imshow(arr_hr[-100][170:320,0:150,:])\n",
    "axs[1,1].imshow(y[0].astype(np.uint8)[340:640,0:300,:])\n",
    "axs[0,0].set_xlabel('120*180*3',fontsize = 60)\n",
    "axs[0,1].set_xlabel('upscaled model of left = 240*360*3',fontsize = 60)\n",
    "axs[1,0].set_xlabel('240*360*3',fontsize = 60)\n",
    "axs[1,1].set_xlabel('upscaled model of left = 720*360*3',fontsize = 60)\n",
    "plt.show();plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "From above, the model is unable to read poorly defined images. (first row above)\n",
    "BUt it is able to resize well-defined images. (second row above)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(plt.imread('file_1_page_0.png'));plt.show()\n",
    "print (plt.imread('file_1_page_0.png')[:5,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv2.filter2D(temp[i], -1, kernel(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "temp = []\n",
    "for i in [ x for x in os.listdir('.') if 'file' in x]:\n",
    "    print (i,np.copy(plt.imread(i)).shape,cv2.resize(np.copy(plt.imread(i)),(525/2,225)).shape)\n",
    "    temp += [np.copy(plt.imread(i)),]\n",
    "    \n",
    "temp = np.stack(temp,0)*255\n",
    "for i in range(len(temp)):\n",
    "    fig, axs = plt.subplots(2,1,figsize=(40,20))\n",
    "    axs[0].imshow(temp[i].astype(np.uint8));\n",
    "    kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "    def kernel(size):\n",
    "        assert size%2 == 1\n",
    "        k = np.zeros((size,size)) - 1\n",
    "        k[size//2,size//2] = -np.sum(k) \n",
    "        print (k)\n",
    "        return k\n",
    "    im = cv2.filter2D(temp[i].astype(np.uint8), -1, kernel(3))\n",
    "    #im = im[:,np.where(im<0)] = 0\n",
    "    #im = im[:,np.where(im>255)] = 255\n",
    "    print (im.shape)\n",
    "    axs[1].imshow(im.astype(np.uint8));#plt.show()\n",
    "    plt.imsave(str(i)+'png',im)\n",
    "    temp[i]= im\n",
    "    plt.show()\n",
    "    die"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "temp = []\n",
    "for i in [ x for x in os.listdir('.') if 'file' in x]:\n",
    "    print (i,np.copy(plt.imread(i)).shape,cv2.resize(np.copy(plt.imread(i)),(525/2,225)).shape)\n",
    "    temp += [np.copy(plt.imread(i)),]\n",
    "    \n",
    "temp = np.stack(temp,0)*255\n",
    "print (temp.shape)\n",
    "upsampled_inp,upsampled_output = get_upsampling_model_2x(temp)\n",
    "up_samplingModel = Model(upsampled_inp,upsampled_output)\n",
    "up_samplingModel.load_weights('./temp_x2_69k.h5')\n",
    "up_samplingModel.summary()\n",
    "z = up_samplingModel.predict(temp,batch_size=1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp[0]= np.stack([th,th,th],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = up_samplingModel.predict(temp[:1],batch_size=1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(z[0].astype(np.uint8));plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "th = cv2.adaptiveThreshold(cv2.cvtColor(z[0].astype(np.uint8),cv2.COLOR_BGR2GRAY), 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 115, 1)\n",
    "cv2.imshow('original',z[0])\n",
    "cv2.imshow('Adaptive threshold',th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imsave('test2.png',(255*(np.mean(z[0],-1)>125)).astype(np.uint8));plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Lets upscale again to 1440 720\n",
    "'''\n",
    "# this gets the output from the model\n",
    "upsampled_inp,upsampled_output = get_upsampling_model_2x(y) \n",
    "\n",
    "# we define the tensor sizes by giving it the example tensor\n",
    "up_model4 = Model(upsampled_inp,upsampled_output)\n",
    "up_model4.load_weights('../../temp_x2_69k.h5')\n",
    "z = up_model4.predict(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Now we try improving the resolution of the original image to 4x the size\n",
    "The model is pretty good at increasing the resolution, and can be used repeatedly.\n",
    "Below is the same image in three zooms. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,3,figsize=(75,75))\n",
    "axs[0,0].imshow(arr_hr[-100])\n",
    "axs[0,1].imshow(y[0].astype(np.uint8))\n",
    "axs[0,2].imshow(z[0].astype(np.uint8))\n",
    "axs[0,0].set_xlabel('ORI 240*360*3',fontsize = 60)\n",
    "axs[0,1].set_xlabel('X2 720*360*3',fontsize = 60)\n",
    "axs[0,2].set_xlabel('X4 1440*720*3',fontsize = 60)\n",
    "\n",
    "axs[1,0].imshow(arr_hr[-100][170:320,0:150,:])\n",
    "axs[1,1].imshow(y[0].astype(np.uint8)[340:640,0:300,:])\n",
    "axs[1,2].imshow(z[0].astype(np.uint8)[340*2:640*2,0:300*2,:])\n",
    "axs[1,0].set_xlabel('ORI 240*360*3',fontsize = 60)\n",
    "axs[1,1].set_xlabel('X2 720*360*3',fontsize = 60)\n",
    "axs[1,2].set_xlabel('X4 1440*720*3',fontsize = 60)\n",
    "\n",
    "axs[2,0].imshow(arr_hr[-100][170:220,0:50,:])\n",
    "axs[2,1].imshow(y[0].astype(np.uint8)[340:440,0:100,:])\n",
    "axs[2,2].imshow(z[0].astype(np.uint8)[340*2:440*2,0:100*2,:])\n",
    "axs[2,0].set_xlabel('ORI 240*360*3',fontsize = 60)\n",
    "axs[2,1].set_xlabel('X2 720*360*3',fontsize = 60)\n",
    "axs[2,2].set_xlabel('X4 1440*720*3',fontsize = 60)\n",
    "plt.savefig('Differrent_resolution.png')\n",
    "plt.show();plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
